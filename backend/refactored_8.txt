import logging
import json
from agents.base_adk_agent import BaseADKAgent
from utils.llm_utils import generate_text_with_gemini
from utils.db_manager import DatabaseManager
from google.generativeai import GenerativeModel as GeminiGenerativeModel

logger = logging.getLogger(__name__)

class CompanionAgent(BaseADKAgent):
    """
    Companion Agent initiates check-ins with patients, adapting tone and detecting emotional cues.
    """
    def __init__(self, db_manager: DatabaseManager, model_name: str, llm_instance: GeminiGenerativeModel):
        self._description = "A friendly agent initiating conversational check-ins for chronic care patients."
        self._instructions_file = "companion_instructions.txt"
        super().__init__(
            db_manager=db_manager,
            model_adk_name=model_name,
            llm_instance=llm_instance,
            name="CompanionAgent",
            description=self._description,
            instructions_file=self._instructions_file,
            tools=[]
        )
        logger.info(f"{self.agent_name} initialized.")

    async def run(self, state: dict) -> dict:
        logger.info(f"{self.agent_name} running for patient {state['patient_id']}")
        patient_state = state.copy()
        latest_input = patient_state.get("latest_patient_input", "")
        conversation_history = patient_state.get("conversation_history", [])
        language_preference = patient_state.get("language_preference", "en")
        accessibility_needs = patient_state.get("accessibility_needs", {})

        if latest_input and (not conversation_history or conversation_history[-1].get("role") != "user"):
            conversation_history.append({"role": "user", "parts": [{"text": latest_input}]})
            patient_state["latest_patient_input"] = ""

        prompt = (
            f"{self.instructions}\n\n"
            f"Patient ID: {patient_state['patient_id']}\n"
            f"Language Preference: {language_preference}\n"
            f"Accessibility Needs: {json.dumps(accessibility_needs)}\n"
            f"Conversation History: {json.dumps(conversation_history[-7:])}\n"
            f"Latest Patient Input: '{latest_input}'\n"
            f"Goal: Engage, detect emotional state, and decide if transition to Adaptive Questionnaire Agent is appropriate."
        )

        try:
            schema = {
                "type": "OBJECT",
                "properties": {
                    "agent_response": {"type": "STRING"},
                    "detected_emotional_state": {"type": "STRING"},
                    "transition_to_adaptive": {"type": "BOOLEAN"},
                    "pro_intro_statement": {"type": "STRING"}
                },
                "required": ["agent_response", "detected_emotional_state", "transition_to_adaptive", "pro_intro_statement"]
            }

            llm_response_str = await generate_text_with_gemini(
                self._llm_instance,
                prompt,
                schema
            )
            llm_response = json.loads(llm_response_str)
            agent_response = llm_response["agent_response"]
            detected_emotional_state = llm_response["detected_emotional_state"]
            transition_to_adaptive = llm_response["transition_to_adaptive"]
            pro_intro_statement = llm_response["pro_intro_statement"]

        except Exception as e:
            logger.error(f"Error in {self.agent_name} for {patient_state['patient_id']}: {e}", exc_info=True)
            agent_response = "Hi! I'm here to check in with you. How are you feeling today?"
            detected_emotional_state = "neutral"
            transition_to_adaptive = False
            pro_intro_statement = ""

        conversation_history.append({"role": "model", "parts": [{"text": agent_response}]})

        patient_state.update({
            "conversation_history": conversation_history,
            "emotional_state": detected_emotional_state,
            "current_agent_flow_flag": "adaptive_questionnaire" if transition_to_adaptive else "companion",
            "pro_intro_statement": pro_intro_statement
        })

        await self.db_manager.save_patient_state(patient_state)

        return patient_state


import logging
import json
from agents.base_adk_agent import BaseADKAgent
from utils.llm_utils import generate_text_with_gemini
from utils.db_manager import DatabaseManager
from utils.security_utils import anonymize_data
from google.generativeai import GenerativeModel as GeminiGenerativeModel

logger = logging.getLogger(__name__)

class AdaptiveQuestionnaireAgent(BaseADKAgent):
    """
    Adaptive Questionnaire Agent personalizes PRO delivery, collects structured PRO data.
    """
    def __init__(self, db_manager: DatabaseManager, model_name: str, llm_instance: GeminiGenerativeModel):
        self._description = "Adaptive agent to deliver personalized PRO questionnaires."
        self._instructions_file = "adaptive_questionnaire_instructions.txt"
        super().__init__(
            db_manager=db_manager,
            model_adk_name=model_name,
            llm_instance=llm_instance,
            name="AdaptiveQuestionnaireAgent",
            description=self._description,
            instructions_file=self._instructions_file,
            tools=[]
        )
        logger.info(f"{self.agent_name} initialized.")

    async def run(self, state: dict) -> dict:
        logger.info(f"{self.agent_name} running for patient {state['patient_id']}")
        patient_state = state.copy()

        conversation_history = patient_state.get("conversation_history", [])
        latest_input = patient_state.get("latest_patient_input", "")
        emotional_state = patient_state.get("emotional_state", "neutral")
        pro_intro_statement = patient_state.get("pro_intro_statement", "")

        if latest_input and (not conversation_history or conversation_history[-1].get("role") != "user"):
            conversation_history.append({"role": "user", "parts": [{"text": latest_input}]})
            patient_state["latest_patient_input"] = ""

        prompt = (
            f"{self.instructions}\n\n"
            f"Patient ID: {patient_state['patient_id']}\n"
            f"Conversation History: {json.dumps(conversation_history[-7:])}\n"
            f"Emotional State: {emotional_state}\n"
            f"PRO Intro Statement: '{pro_intro_statement}'\n"
            f"Latest Patient Input: '{latest_input}'\n"
            f"Goal: Generate next adaptive PRO question or conclude questionnaire."
        )

        try:
            schema = {
                "type": "OBJECT",
                "properties": {
                    "agent_question": {"type": "STRING"},
                    "detected_emotional_state": {"type": "STRING"},
                    "pro_data_extracted": {
                        "type": "OBJECT",
                        "properties": {
                            "pain_level": {"type": ["INTEGER", "null"]},
                            "fatigue_level": {"type": ["INTEGER", "null"]},
                            "mood_description": {"type": ["STRING", "null"]},
                            "medication_adherence_issue": {"type": ["BOOLEAN", "null"]},
                            "general_wellbeing": {"type": ["STRING", "null"]}
                        }
                    },
                    "is_questionnaire_complete": {"type": "BOOLEAN"}
                },
                "required": ["agent_question", "detected_emotional_state", "pro_data_extracted", "is_questionnaire_complete"]
            }

            llm_response_str = await generate_text_with_gemini(
                self._llm_instance,
                prompt,
                schema
            )
            llm_response = json.loads(llm_response_str)

            agent_question = llm_response["agent_question"]
            detected_emotional_state = llm_response["detected_emotional_state"]
            pro_data_extracted = llm_response["pro_data_extracted"]
            is_complete = llm_response["is_questionnaire_complete"]

        except Exception as e:
            logger.error(f"Error in {self.agent_name} for {patient_state['patient_id']}: {e}", exc_info=True)
            agent_question = "Sorry, I didn't quite get that. Could you please rephrase?"
            detected_emotional_state = emotional_state
            pro_data_extracted = {}
            is_complete = False

        conversation_history.append({"role": "model", "parts": [{"text": agent_question}]})

        patient_state.update({
            "conversation_history": conversation_history,
            "emotional_state": detected_emotional_state,
            "current_agent_flow_flag": "trend_monitoring" if is_complete else "adaptive_questionnaire"
        })

        if is_complete and pro_data_extracted:
            anonymized_data = anonymize_data(pro_data_extracted)
            await self.db_manager.save_pro_data(patient_state["patient_id"], anonymized_data, self.agent_name)
            patient_state["latest_pro_data_collected"] = anonymized_data

        await self.db_manager.save_patient_state(patient_state)

        return patient_state

import logging
import json
from datetime import datetime
from agents.base_adk_agent import BaseADKAgent
from utils.llm_utils import generate_text_with_gemini
from utils.db_manager import DatabaseManager
from utils.security_utils import anonymize_data
from google.generativeai import GenerativeModel as GeminiGenerativeModel

logger = logging.getLogger(__name__)

class TrendMonitoringAgent(BaseADKAgent):
    """
    Analyzes historical PRO data for trends, flags risks, and generates summaries/alerts.
    """
    def __init__(self, db_manager: DatabaseManager, model_name: str, llm_instance: GeminiGenerativeModel):
        self._description = "Agent that analyzes historical PRO data to detect trends and risks."
        self._instructions_file = "trend_monitoring_instructions.txt"
        super().__init__(
            db_manager=db_manager,
            model_adk_name=model_name,
            llm_instance=llm_instance,
            name="TrendMonitoringAgent",
            description=self._description,
            instructions_file=self._instructions_file,
            tools=[]
        )
        logger.info(f"{self.agent_name} initialized.")

    async def run(self, state: dict) -> dict:
        logger.info(f"{self.agent_name} running for patient {state['patient_id']}")
        patient_state = state.copy()
        patient_id = patient_state["patient_id"]

        raw_pro_data = await self.db_manager.get_pro_data(patient_id)
        historical_pro_data = [anonymize_data(entry["data_elements"]) for entry in raw_pro_data]

        prompt = (
            f"{self.instructions}\n\n"
            f"Patient ID: {patient_id}\n"
            f"Historical PRO Data (de-identified): {json.dumps(historical_pro_data)}\n"
            f"Goal: Analyze for trends and risk signals. Provide summary and alerts."
        )

        try:
            schema = {
                "type": "OBJECT",
                "properties": {
                    "alert_type": {"type": "STRING", "enum": ["risk_signal", "trend_summary", "no_alert"]},
                    "severity": {"type": "STRING", "enum": ["low", "medium", "high", "critical", "none"]},
                    "summary_text": {"type": "STRING"}
                },
                "required": ["alert_type", "severity", "summary_text"]
            }

            llm_response_str = await generate_text_with_gemini(
                self._llm_instance,
                prompt,
                schema
            )
            llm_response = json.loads(llm_response_str)

            alert_type = llm_response["alert_type"]
            severity = llm_response["severity"]
            summary_text = llm_response["summary_text"]

        except Exception as e:
            logger.error(f"Error in {self.agent_name} for {patient_id}: {e}", exc_info=True)
            alert_type = "no_alert"
            severity = "none"
            summary_text = "Trend analysis currently unavailable."

        insights = {
            "alert_type": alert_type,
            "severity": severity,
            "summary_text": summary_text
        }

        await self.db_manager.save_insight_alert(
            patient_id,
            alert_type,
            severity,
            summary_text,
            self.agent_name
        )

        patient_state.update({
            "current_agent_flow_flag": "completed",
            "last_check_in": datetime.now()
        })

        await self.db_manager.save_patient_state(patient_state)

        return patient_state

import json
import logging

async def generate_text_with_gemini(llm_instance, prompt_text, response_schema):
    """
    Calls the Gemini LLM API with the prompt and JSON schema.
    Returns the text response as a string.
    """
    try:
        response = await llm_instance.generate_text(
            prompt=prompt_text,
            schema=response_schema,
            temperature=0.7,
            max_tokens=512
        )
        return response.text
    except Exception as e:
        logging.error(f"LLM generation error: {e}")
        raise e


import asyncpg
import logging
import os

class DatabaseManager:
    def __init__(self):
        self.pool = None

    async def connect(self):
        self.pool = await asyncpg.create_pool(
            dsn=os.getenv("DATABASE_URL"),
            max_size=20,
            command_timeout=60
        )
        logging.info("Database connection pool created.")

    async def save_patient_state(self, patient_state: dict):
        async with self.pool.acquire() as conn:
            await conn.execute(
                """
                INSERT INTO patient_states (patient_id, state_json, last_updated)
                VALUES ($1, $2, NOW())
                ON CONFLICT (patient_id) DO UPDATE
                SET state_json = EXCLUDED.state_json,
                    last_updated = EXCLUDED.last_updated
                """,
                patient_state["patient_id"],
                json.dumps(patient_state)
            )

    async def save_pro_data(self, patient_id: str, pro_data: dict, agent_name: str):
        async with self.pool.acquire() as conn:
            await conn.execute(
                """
                INSERT INTO pro_data (patient_id, data_elements, collected_at, collected_by)
                VALUES ($1, $2, NOW(), $3)
                """,
                patient_id,
                json.dumps(pro_data),
                agent_name
            )

    async def get_pro_data(self, patient_id: str):
        async with self.pool.acquire() as conn:
            return await conn.fetch(
                "SELECT data_elements, collected_at FROM pro_data WHERE patient_id = $1 ORDER BY collected_at DESC",
                patient_id
            )

    async def save_insight_alert(self, patient_id: str, alert_type: str, severity: str, summary_text: str, agent_name: str):
        async with self.pool.acquire() as conn:
            await conn.execute(
                """
                INSERT INTO insight_alerts (patient_id, alert_type, severity, summary_text, generated_at, generated_by)
                VALUES ($1, $2, $3, $4, NOW(), $5)
                """,
                patient_id,
                alert_type,
                severity,
                summary_text,
                agent_name
            )

import os
import logging
from fastapi import FastAPI, HTTPException, Depends, Header, Request
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
from agents.companion_agent import CompanionAgent
from agents.adaptive_questionnaire_agent import AdaptiveQuestionnaireAgent
from agents.trend_monitoring_agent import TrendMonitoringAgent
from utils.db_manager import DatabaseManager
from google.generativeai import client as gclient
from typing import Optional

app = FastAPI()

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # Adjust for production frontend URL(s)
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

db_manager = DatabaseManager()

# Initialize Gemini API client
API_KEY = os.getenv("GOOGLE_ADK_SECRET_KEY")
if not API_KEY:
    raise RuntimeError("Missing GOOGLE_ADK_SECRET_KEY environment variable")

gclient.configure(api_key=API_KEY)

@app.on_event("startup")
async def startup_event():
    await db_manager.connect()
    # Initialize agents here with db_manager and Gemini model instances
    global companion_agent, adaptive_agent, trend_agent
    companion_agent = CompanionAgent(db_manager, "companion-adk", gclient)
    adaptive_agent = AdaptiveQuestionnaireAgent(db_manager, "adaptive-adk", gclient)
    trend_agent = TrendMonitoringAgent(db_manager, "trend-adk", gclient)
    logging.info("Agents initialized and database connected.")

# Auth Dependency Example (simple header token check)
async def verify_secret_key(x_secret_key: Optional[str] = Header(None)):
    if x_secret_key != API_KEY:
        raise HTTPException(status_code=403, detail="Forbidden")

# Pydantic models for requests/responses
class CheckInRequest(BaseModel):
    patient_id: str
    latest_patient_input: str

class AgentResponse(BaseModel):
    state: dict

@app.post("/api/companion", response_model=AgentResponse, dependencies=[Depends(verify_secret_key)])
async def companion_endpoint(request: CheckInRequest):
    state = {
        "patient_id": request.patient_id,
        "latest_patient_input": request.latest_patient_input,
        "conversation_history": [],
        "current_agent_flow_flag": "companion"
    }
    updated_state = await companion_agent.run(state)
    return {"state": updated_state}

@app.post("/api/adaptive_questionnaire", response_model=AgentResponse, dependencies=[Depends(verify_secret_key)])
async def adaptive_questionnaire_endpoint(request: CheckInRequest):
    # Load existing patient state from DB (simplified for demo)
    patient_state = await db_manager.get_patient_state(request.patient_id)
    if not patient_state:
        raise HTTPException(404, "Patient state not found")
    patient_state["latest_patient_input"] = request.latest_patient_input
    updated_state = await adaptive_agent.run(patient_state)
    return {"state": updated_state}

@app.post("/api/trend_monitoring", response_model=AgentResponse, dependencies=[Depends(verify_secret_key)])
async def trend_monitoring_endpoint(request: CheckInRequest):
    patient_state = await db_manager.get_patient_state(request.patient_id)
    if not patient_state:
        raise HTTPException(404, "Patient state not found")
    updated_state = await trend_agent.run(patient_state)
    return {"state": updated_state}

You are the Companion Agent, a friendly and empathetic AI assistant designed to initiate
conversational check-ins with patients managing chronic conditions. Your primary goal
is to engage the patient in a natural, supportive conversation, adapt to their emotional state,
and subtly assess their readiness to discuss their health in more detail (i.e., proceed to a PRO questionnaire).

Your Persona:
- Empathetic and caring.
- Conversational and natural, avoiding overly clinical language initially.
- Patient and reassuring.
- Aware of and adaptable to patient's language and accessibility needs.

Your Tasks:
1.  Initiate Check-in: Start with a warm, open-ended greeting.
2.  Adapt Tone/Complexity: Adjust your language and complexity based on inferred comprehension and historical interaction patterns.
3.  Detect Emotional Cues: Pay close attention to keywords, tone (if audio input were available), and sentiment to detect emotional states (e.g., tired, sad, anxious, frustrated, positive). Respond with empathy and reassurance as appropriate.
4.  Guide Conversation: Gently steer the conversation towards general well-being and health without being intrusive.
5.  Assess Readiness for PROs: Determine if the patient seems open and willing to discuss specific health aspects. If they express any discomfort or resistance, do not push them. Instead, offer reassurance and suggest connecting later or providing general well-being tips.
6.  Transition Decision: Based on the conversation, decide whether to transition the patient to the "Adaptive Questionnaire Agent" or continue the conversation as the Companion Agent.

Output Format (JSON):
You must output a JSON object with the following keys:
- agent_response: The conversational text you will send back to the patient.
- detected_emotional_state: Your assessment of the patient's current emotional state (e.g., "neutral", "happy", "tired", "anxious", "frustrated", "sad").
- transition_to_adaptive: A boolean (true/false) indicating if the patient should now be transitioned to the Adaptive Questionnaire Agent. Set to true if they express readiness or a clear opening to discuss symptoms/well-being.
- pro_intro_statement: A brief, gentle statement to introduce the idea of more specific health questions, to be used if transition_to_adaptive is true.


# Multi-Agent PRO Healthcare System Backend

## Overview
This backend implements a multi-agent AI system for capturing and analyzing Patient Reported Outcomes (PROs) for chronic care patients. The system includes:

- Companion Agent: Initiates empathetic patient check-ins.
- Adaptive Questionnaire Agent: Delivers personalized PRO questionnaires.
- Trend Monitoring Agent: Analyzes PRO data trends and alerts clinicians.

## Technologies
- FastAPI for API layer
- PostgreSQL for persistent storage
- Google ADK Gemini generative AI for agents
- Asyncpg for async DB connection
- Python 3.9+

## Setup

### Prerequisites
- PostgreSQL instance running
- Python 3.9+
- Google ADK API key

### Environment Variables
Create a `.env` file with:


### Database Setup
Run the SQL schema located in `db/schema.sql` (not included here) to create necessary tables:
- patient_states
- pro_data
- insight_alerts

### Install Dependencies

```bash

pip install -r requirements.txt



fastapi==0.95.2
uvicorn[standard]==0.22.0
asyncpg==0.27.0
databases[postgresql]==0.6.3
psycopg[binary]==2.9.6
sqlalchemy==2.0.19
python-dotenv==1.0.0
pydantic==1.10.12
httpx==0.24.1
alembic==1.11.1
python-multipart==0.0.6
passlib[bcrypt]==1.7.4
python-jose[cryptography]==3.3.0
pytest==7.4.0
pytest-asyncio==0.21.0
